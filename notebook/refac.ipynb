{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kagglehub\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import make_scorer, fbeta_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c03caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "POPULARITY_THRESHOLD = 85\n",
    "SEED = 42\n",
    "BETA = 1.5\n",
    "\n",
    "RECALL_THRESHOLD = 0.7\n",
    "PRECISION_THRESHOLD = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23eb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "data_path = kagglehub.dataset_download(\"amitanshjoshi/spotify-1million-tracks\")\n",
    "\n",
    "print(\"Path to dataset files:\", data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832dacda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I. Load data\n",
    "data = pd.read_csv(f\"{data_path}/spotify_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d78f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['artist_name'].nunique(), data['track_name'].nunique(), data['track_id'].nunique()\n",
    "artists_popularity = data.groupby('artist_name')['popularity'].mean().sort_values(ascending=False)\n",
    "# add columns names (artist_name, average_popularity)\n",
    "artists_popularity = artists_popularity.reset_index()\n",
    "artists_popularity.columns = ['artist_name', 'average_popularity']\n",
    "\n",
    "artists_popularity = artists_popularity[artists_popularity['average_popularity'] > 0]\n",
    "# get the 20 top artists based on average popularity of their songs\n",
    "top_artists = data.groupby('artist_name')['popularity'].mean().sort_values(ascending=False).head(20)\n",
    "top_artists.plot(kind='bar', figsize=(12, 6), title='Top 20 Artists by Average Popularity')\n",
    "# add feature for every song, number of songs from the same artist\n",
    "data['artist_song_count'] = data.groupby('artist_name')['track_id'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = data['year'].astype(int)\n",
    "yearly_thresholds = data.groupby('year')['popularity'].quantile(POPULARITY_THRESHOLD / 100).to_dict()\n",
    "data['verdict'] = data.apply(lambda row: 1 if row['popularity'] >= yearly_thresholds[row['year']] else 0, axis=1)\n",
    "data['verdict'].value_counts()\n",
    "yearly_thresholds\n",
    "# add a red median line\n",
    "median_popularity = data['popularity'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verdict 1 percentage\n",
    "verdict_1_percentage = (data['verdict'].sum() / data.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the quantiles for duration_ms\n",
    "Q1 = data['duration_ms'].quantile(0.25)\n",
    "Q3 = data['duration_ms'].quantile(0.75)\n",
    "Q4 = data['duration_ms'].quantile(0.95)\n",
    "IQR = Q3 - Q1\n",
    "print(f\"Q1: {Q1}, Q3: {Q3}, Q4: {Q4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2480b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add feature normal vs long duration\n",
    "data['long_duration'] = data['duration_ms'].apply(lambda x: 1 if x > Q4 else 0)\n",
    "\n",
    "# add feature normal vs short duration\n",
    "data['short_duration'] = data['duration_ms'].apply(lambda x: 1 if x < Q1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e7aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of tracks with 0 popularity\n",
    "num_zero_popularity = data[data['popularity'] == 0].shape[0]\n",
    "percentage_zero_popularity = (num_zero_popularity / data.shape[0]) * 100\n",
    "print(f\"Percentage of tracks with 0 popularity: {percentage_zero_popularity:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec7bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbr of tracks with popularity >=95\n",
    "num_max_popularity = data[data['popularity'] >= 95].shape[0]\n",
    "print(f\"Number of tracks with popularity >=95: {num_max_popularity}\")\n",
    "percentage_max_popularity = (num_max_popularity / data.shape[0]) * 100\n",
    "print(f\"Percentage of tracks with popularity >=95: {percentage_max_popularity:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5813dd32",
   "metadata": {},
   "source": [
    "# III. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d536028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model that randomly predicts the popularity based on the distribution of popularity in the dataset\n",
    "\n",
    "mean_popularity = data['popularity'].mean()\n",
    "std_popularity = data['popularity'].std()\n",
    "\n",
    "data['random_popularity'] = np.random.normal(mean_popularity, std_popularity, size=len(data))\n",
    "data['random_verdict'] = (data['random_popularity'] > POPULARITY_THRESHOLD).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_encode_column(df, column_name) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One-hot encode a categorical column in the dataframe.\n",
    "    \"\"\"\n",
    "    one_hot = pd.get_dummies(df[column_name], prefix=column_name).astype(int)\n",
    "    df = df.drop(column_name, axis=1)\n",
    "    df = df.join(one_hot)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc0eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple decision tree model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "features = data.drop(columns=['popularity', 'verdict', 'random_popularity', 'random_verdict'])\n",
    "features = features.drop(columns=['Unnamed: 0', 'artist_name', 'track_name', 'track_id', 'year'])\n",
    "features.dropna(inplace=True)\n",
    "\n",
    "features = hot_encode_column(features, 'genre')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data['verdict'], test_size=0.2, random_state=SEED)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=SEED, class_weight='balanced', max_depth=5)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb1dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a random forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=SEED, class_weight='balanced', n_estimators=100, max_depth=5, max_features='sqrt', min_samples_split=10)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f80f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop(columns=['popularity', 'verdict', 'random_popularity', 'random_verdict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee89bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier(random_state=SEED, \n",
    "                          objective='binary:logistic',\n",
    "                          colsample_bytree=0.8, \n",
    "                          learning_rate=0.1, \n",
    "                          max_depth=6, \n",
    "                          n_estimators=1000, \n",
    "                          subsample=0.9, \n",
    "                          scale_pos_weight=6)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf645b2",
   "metadata": {},
   "source": [
    "# Calculate the best Threshold for popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17fbdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_score(y_true, y_pred):\n",
    "    prec = precision_score(y_true, y_pred, pos_label=1)\n",
    "    rec  = recall_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "    # Hard constraints\n",
    "    if rec < RECALL_THRESHOLD or prec < PRECISION_THRESHOLD:\n",
    "        return 0.0\n",
    "\n",
    "    # Smooth reward zone once constraints are satisfied\n",
    "    # You can tweak the weights\n",
    "    return 0.5 * prec + 0.5 * rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5255db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that run an xgboost model for all thresholds from 50 to 70 and return the best threshold based on the average of recalls\n",
    "def find_best_threshold(data, start=50, end=70):\n",
    "    best_threshold = start\n",
    "    best_recall_avg = 0.0\n",
    "    delta_recalls = 0.0\n",
    "    score = 0.0\n",
    "    best_recall_avg_lst = []\n",
    "    deltas_lst = []\n",
    "    scores_lst = []\n",
    "    scorer = make_scorer(constrained_score, greater_is_better=True)\n",
    "\n",
    "    for threshold in range(start, end + 1):\n",
    "        print(f\"Evaluating threshold: {threshold}\")\n",
    "        data['verdict'] = (data['popularity'] > threshold).astype(int)\n",
    "        X = data.drop(columns=['popularity', 'verdict', 'random_popularity', 'random_verdict'])\n",
    "        X = X.drop(columns=['Unnamed: 0', 'artist_name', 'track_name', 'track_id', 'year'])\n",
    "        X = hot_encode_column(X, 'genre')\n",
    "        y = data['verdict']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "        xgb_model = XGBClassifier(random_state=SEED, \n",
    "                                  objective='binary:logistic',\n",
    "                                  colsample_bytree=0.8, \n",
    "                                  learning_rate=0.1,\n",
    "                                  max_depth=6, \n",
    "                                  n_estimators=600, \n",
    "                                  subsample=0.9, \n",
    "                                  scale_pos_weight=30, n_jobs=-1)\n",
    "        \n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        predictions = xgb_model.predict(X_test)\n",
    "\n",
    "        report = classification_report(y_test, predictions, output_dict=True)\n",
    "        recall_avg = (report['1']['recall'] + report['0']['recall']) / 2\n",
    "        delta_recalls = report['1']['recall'] - report['0']['recall']\n",
    "        score = scorer(xgb_model, X_test, y_test)\n",
    "\n",
    "        best_recall_avg_lst.append(recall_avg)\n",
    "        deltas_lst.append(delta_recalls)\n",
    "        scores_lst.append(score)\n",
    "\n",
    "        print(f\"Threshold: {threshold}, Average Recall: {recall_avg:.2f}, Delta Recalls: {delta_recalls:.2f}, Score: {score:.2f}\")\n",
    "\n",
    "        if recall_avg > best_recall_avg:\n",
    "            best_recall_avg = recall_avg\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold, best_recall_avg, best_recall_avg_lst, deltas_lst, scores_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788cfcf5",
   "metadata": {},
   "source": [
    "## SearchGridCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78841cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=SEED, \n",
    "                          objective='binary:logistic',\n",
    "                          n_jobs=-1)\n",
    "\n",
    "# scorer = make_scorer(fbeta_score, beta=2)\n",
    "scorer = make_scorer(constrained_score, greater_is_better=True)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [600, 800, 1000],\n",
    "    'max_depth': [6],\n",
    "    'learning_rate': [0.1],\n",
    "    'subsample': [0.9],\n",
    "    'colsample_bytree': [0.6, 0.8],\n",
    "    'scale_pos_weight': [6, 12]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='recall_macro', cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86165e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
